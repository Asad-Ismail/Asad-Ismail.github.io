<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>
  Machine Learning Model deployment · Asad Ismail
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Asad Ismail">
<meta name="description" content="Machine learning deployment">
<meta name="keywords" content="blog,developer,personal">
<meta name="fediverse:creator" content="" />


  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Machine Learning Model deployment">
  <meta name="twitter:description" content="Machine learning deployment">

<meta property="og:url" content="http://localhost:1313/posts/ml_deployment/">
  <meta property="og:site_name" content="Asad Ismail">
  <meta property="og:title" content="Machine Learning Model deployment">
  <meta property="og:description" content="Machine learning deployment">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-16T00:36:50+02:00">
    <meta property="article:modified_time" content="2024-08-16T00:36:50+02:00">




<link rel="canonical" href="http://localhost:1313/posts/ml_deployment/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.css" media="screen">






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.css" media="screen">
  



 




<link rel="icon" type="image/svg+xml" href="/img/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/img/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/img/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://localhost:1313/">
      Asad Ismail
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/">Home</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/projects/">Projects</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://localhost:1313/posts/ml_deployment/">
              Machine Learning Model deployment
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2024-08-16T00:36:50&#43;02:00">
                August 16, 2024
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              6-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        <h2 id="machine-learning-model-deployment">
  Machine Learning Model Deployment
  <a class="heading-link" href="#machine-learning-model-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>After successfully building your ML/DL model in a notebook, the next critical step is deployment—getting your model out into the world to serve customers. There are two primary avenues for deploying machine learning models:</p>
<ol>
<li><strong>Edge Deployment</strong></li>
<li><strong>Cloud Deployment</strong></li>
</ol>
<h3 id="edge-deployment">
  Edge Deployment
  <a class="heading-link" href="#edge-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Edge deployment involves placing the model directly on the device where it will be used. This approach is particularly valuable in situations where:</p>
<ol>
<li><strong>Latency Requirements</strong>: Real-time deployment and control are essential, and network latency is unacceptably high.</li>
<li><strong>No Network Availability</strong>: The model must operate in environments without internet connectivity.</li>
</ol>
<h4 id="examples">
  Examples:
  <a class="heading-link" href="#examples">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<ul>
<li>Deploying perception models for advanced driving assistance systems (ADAS) or self-driving cars, where split-second decisions are critical.</li>
<li>Deploying ML models on machinery in remote fields with no internet connectivity, where immediate, on-the-spot results are required.</li>
</ul>
<h3 id="cloud-deployment">
  Cloud Deployment
  <a class="heading-link" href="#cloud-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Cloud deployment allows you to leverage the power and flexibility of cloud computing. This can be done in two main ways:</p>
<ol>
<li>
<p><strong>Self-Managed Cloud Deployments</strong>: Using tools like KFServing or KServe, organizations can deploy and manage ML models in the cloud on their own. This option provides extensive control but comes with a learning curve, making it more suitable for larger companies with the necessary resources.</p>
</li>
<li>
<p><strong>Managed Cloud Services</strong>: Alternatively, you can opt for managed services from major cloud providers like AWS, Azure, and Google Cloud Platform (GCP). These services simplify deployment and management, though they may have certain limitations.</p>
</li>
</ol>
<p>Below is a visual representation of the market share among major cloud providers for ML model deployment. The data is from 2022, and it&rsquo;s possible that Azure, with its partnership with OpenAI, has already surpassed AWS.</p>
<div style="text-align: center;">
    <img src="/images/cloud_providers.png" alt="Cloud Market Share" style="max-width: 50%; height: auto;" />
    
    <p style="text-align: center; font-style: italic; color: #666;">Cloud Market share @2022</p>
    
</div>


<h3 id="focus-on-aws-for-ml-deployment">
  Focus on AWS for ML Deployment
  <a class="heading-link" href="#focus-on-aws-for-ml-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>In this discussion, we&rsquo;ll focus on deploying machine learning models using AWS, although Azure and GCP offer similar services. Below are the key types of ML deployments available on AWS.</p>
<h4 id="aws-batch-processing">
  AWS Batch Processing
  <a class="heading-link" href="#aws-batch-processing">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>AWS Batch is a versatile batch processing service that enables you to run large-scale computing workloads on AWS.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Versatility:</strong> Suitable for a wide range of batch jobs, beyond just machine learning.</li>
<li><strong>Custom Docker Containers:</strong> Supports running jobs within custom Docker images, offering flexibility in the environment setup.</li>
<li><strong>Broad Use Cases:</strong> Applicable for tasks like data processing, image rendering, and large-scale simulations.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>General-Purpose Nature:</strong> Not specifically optimized for machine learning tasks, which might require additional setup or configuration.</li>
<li><strong>Potentially Complex Configuration:</strong> Depending on the use case, configuring batch jobs might require more detailed setup compared to services tailored to specific tasks like ML.</li>
</ul>
<h4 id="sagemaker-batch-transform">
  SageMaker Batch Transform
  <a class="heading-link" href="#sagemaker-batch-transform">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>SageMaker Batch Transform is designed specifically for batch inference tasks in machine learning.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>ML-Specific:</strong> Optimized for batch processing of machine learning model inferences, ensuring a streamlined workflow.</li>
<li><strong>Seamless Integration:</strong> Easily integrates with other SageMaker services like training and deployment, simplifying the end-to-end ML process.</li>
<li><strong>High Throughput:</strong> Designed for large-scale ML tasks, allowing for efficient processing of extensive datasets.</li>
<li><strong>Event-Driven:</strong> Can be triggered by S3 events, enabling automated processing of new data as it arrives.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Limited to ML Use Cases:</strong> Not suitable for non-ML batch processing tasks, which limits its flexibility compared to AWS Batch.</li>
<li><strong>Dependent on SageMaker Ecosystem:</strong> While integration is an advantage, it also means that it&rsquo;s less flexible if you need to use non-SageMaker services.</li>
</ul>
<h3 id="real-time-endpoints">
  Real-Time Endpoints
  <a class="heading-link" href="#real-time-endpoints">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Real-Time Endpoints in SageMaker are used for scenarios where immediate inference is required. They provide a scalable and low-latency way to get predictions as soon as data is sent to the model.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Low Latency:</strong> Delivers immediate responses, making it ideal for real-time applications.</li>
<li><strong>API Integration:</strong> Can be easily invoked via HTTP requests, making it straightforward to integrate into existing applications.</li>
<li><strong>Auto-Scaling:</strong> Automatically adjusts the number of instances based on demand, ensuring performance consistency.</li>
<li><strong>Flexible Instance Options:</strong> Offers both CPU and GPU instances, catering to different performance needs.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Payload Size Limit:</strong> The maximum input size is 6 MB, which might require you to downscale or compress larger inputs.</li>
<li><strong>Timeout Constraints:</strong> The model needs to return results within 60 seconds, which can be limiting for more complex models.</li>
</ul>
<h3 id="asynchronous-endpoints">
  Asynchronous Endpoints
  <a class="heading-link" href="#asynchronous-endpoints">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Asynchronous Endpoints are designed for scenarios where inference does not need to be instantaneous. They allow for processing larger payloads and longer processing times, making them suitable for more complex or batch-like tasks.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Handles Larger Payloads:</strong> Supports input sizes up to 1 GB, which is significantly higher than real-time endpoints.</li>
<li><strong>Extended Processing Time:</strong> Allows up to 15 minutes for inference, accommodating more complex computations.</li>
<li><strong>Queue Inputs:</strong> Inputs can be queued, which is useful for handling large volumes of data that do not require immediate processing.</li>
<li><strong>Auto-Scaling:</strong> Automatically scales based on demand, similar to real-time endpoints.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Near Real-Time Performance:</strong> While it supports larger payloads and longer processing times, the latency is higher compared to real-time endpoints.</li>
<li><strong>More Complex Workflow:</strong> Managing asynchronous jobs might require more careful monitoring and management compared to the simpler real-time endpoint approach.</li>
</ul>
<h3 id="sagemaker-serverless-inference">
  Sagemaker Serverless Inference
  <a class="heading-link" href="#sagemaker-serverless-inference">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Serverless Inference enables to deploy and scale ML models without configuring or managing any of the underlying infrastructure. We can use serveless inference for cpu only loads for requests where there is idle time between requests.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Autoscaling:</strong> Serverless Inference scales your endpoint down to 0, helping you to minimize your costs.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>CPU inference:</strong> Currently it supports only cpu compute.</li>
<li><strong>Cold start:</strong> We have to tolerate cold start for intial requests.</li>
</ul>
<p>More details here <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html"  class="external-link" target="_blank" rel="noopener">https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html</a></p>
<p>I will now go over throw show the code for Sagemaker Batch Transform, Real Time endpoints and Async Endpoints with the example of Instance segmentation using Detectron2 MaskRCNN implementation. Why instance segemntation example? I believe it is not covered as much as object detection and classification when it has the most utility in industry for computer vision tasks as it combine object detection with segmentation giving the class information along with precise object boundary for measurements. I will train the model using detectron2 example. Train it on sample dataset provided in detectron2 repo, the focus is not on the training but deployment but for the sake of completeness I will go over through complete example.</p>
<h2 id="instance-segmenation-training-and-inference-locally">
  Instance Segmenation Training and inference Locally
  <a class="heading-link" href="#instance-segmenation-training-and-inference-locally">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>An example of basic training and inference of detectron2 maskrcnn model is given in this repo <a href="https://github.com/facebookresearch/detectron2?tab=readme-ov-file"  class="external-link" target="_blank" rel="noopener">https://github.com/facebookresearch/detectron2?tab=readme-ov-file</a></p>
<p>A simple example of training maskrcnn model</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">detectron2.config</span> <span style="color:#ff7b72">import</span> get_cfg
</span></span><span style="display:flex;"><span>cfg <span style="color:#ff7b72;font-weight:bold">=</span> get_cfg()
</span></span><span style="display:flex;"><span>model_arch<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml&#34;</span>
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>merge_from_file(model_zoo<span style="color:#ff7b72;font-weight:bold">.</span>get_config_file(model_arch))
</span></span><span style="display:flex;"><span>trainer <span style="color:#ff7b72;font-weight:bold">=</span> DefaultTrainer(cfg) 
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff7b72;font-weight:bold">.</span>resume_or_load(resume<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#79c0ff">False</span>)
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff7b72;font-weight:bold">.</span>train()
</span></span></code></pre></div><p>and we can use this model to perorm inference like below</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">detectron2.config</span> <span style="color:#ff7b72">import</span> get_cfg
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">detectron2.engine</span> <span style="color:#ff7b72">import</span> DefaultPredictor
</span></span><span style="display:flex;"><span>cfg <span style="color:#ff7b72;font-weight:bold">=</span> get_cfg()
</span></span><span style="display:flex;"><span>model_arch<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml&#34;</span>
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>merge_from_file(model_zoo<span style="color:#ff7b72;font-weight:bold">.</span>get_config_file(model_arch))
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>MODEL<span style="color:#ff7b72;font-weight:bold">.</span>WEIGHTS <span style="color:#ff7b72;font-weight:bold">=</span> os<span style="color:#ff7b72;font-weight:bold">.</span>path<span style="color:#ff7b72;font-weight:bold">.</span>join(cfg<span style="color:#ff7b72;font-weight:bold">.</span>OUTPUT_DIR, <span style="color:#a5d6ff">&#34;model_final.pth&#34;</span>)  <span style="color:#8b949e;font-style:italic"># path to the model we just trained</span>
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>MODEL<span style="color:#ff7b72;font-weight:bold">.</span>ROI_HEADS<span style="color:#ff7b72;font-weight:bold">.</span>SCORE_THRESH_TEST <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">0.7</span>   <span style="color:#8b949e;font-style:italic"># set a custom testing threshold</span>
</span></span><span style="display:flex;"><span>predictor <span style="color:#ff7b72;font-weight:bold">=</span> DefaultPredictor(cfg)
</span></span><span style="display:flex;"><span>im <span style="color:#ff7b72;font-weight:bold">=</span> cv2<span style="color:#ff7b72;font-weight:bold">.</span>imread(d[<span style="color:#a5d6ff">&#34;file_name&#34;</span>])
</span></span><span style="display:flex;"><span>outputs <span style="color:#ff7b72;font-weight:bold">=</span> predictor(im)
</span></span></code></pre></div><p>see full example here <a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=U5LhISJqWXgM"  class="external-link" target="_blank" rel="noopener">https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=U5LhISJqWXgM</a></p>
<p>Once we have trained, evaluated and tested the model we want to deploy the model. We will show the most general case of deployment to build our own custom container so it is aplicable to pretty much all ML models not tied down to specific container proided by sagemaker.</p>
<h2 id="aws-batch-transform">
  AWS Batch Transform
  <a class="heading-link" href="#aws-batch-transform">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>

      </div>


      <footer>
        


        
        
        
        
        

        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.js"></script>
  

  

  


  
  



  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
