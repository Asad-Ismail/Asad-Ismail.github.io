<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  Machine Learning Model deployment · Asad Ismail
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Asad Ismail">
<meta name="description" content="Machine learning deployment">
<meta name="keywords" content="blog,developer,personal">
<meta name="fediverse:creator" content="" />


  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Machine Learning Model deployment">
  <meta name="twitter:description" content="Machine learning deployment">

<meta property="og:url" content="https://Asad-Ismail.github.io/posts/ml_deployment/">
  <meta property="og:site_name" content="Asad Ismail">
  <meta property="og:title" content="Machine Learning Model deployment">
  <meta property="og:description" content="Machine learning deployment">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-16T00:36:50+02:00">
    <meta property="article:modified_time" content="2024-08-16T00:36:50+02:00">




<link rel="canonical" href="https://Asad-Ismail.github.io/posts/ml_deployment/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.e927f7340e309d76dcb8fda85f1531ae7341aa9cd0b7f3ab77885dae77b1a0a2.css" integrity="sha256-6Sf3NA4wnXbcuP2oXxUxrnNBqpzQt/Ord4hdrnexoKI=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css" integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/svg+xml" href="/img/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/img/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/img/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="https://Asad-Ismail.github.io/">
      Asad Ismail
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/">Home</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/projects/">Projects</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://Asad-Ismail.github.io/posts/ml_deployment/">
              Machine Learning Model deployment
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2024-08-16T00:36:50&#43;02:00">
                August 16, 2024
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              10-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        <h2 id="machine-learning-model-deployment">
  Machine Learning Model Deployment
  <a class="heading-link" href="#machine-learning-model-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>After successfully building your ML/DL model in a notebook, the next critical step is deployment—getting your model out into the world to serve customers. There are two primary avenues for deploying machine learning models:</p>
<ol>
<li><strong>Edge Deployment</strong></li>
<li><strong>Cloud Deployment</strong></li>
</ol>
<h3 id="edge-deployment">
  Edge Deployment
  <a class="heading-link" href="#edge-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Edge deployment involves placing the model directly on the device where it will be used. This approach is particularly valuable in situations where:</p>
<ol>
<li><strong>Latency Requirements</strong>: Real-time deployment and control are essential, and network latency is unacceptably high.</li>
<li><strong>No Network Availability</strong>: The model must operate in environments without internet connectivity.</li>
</ol>
<h4 id="examples">
  Examples:
  <a class="heading-link" href="#examples">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<ul>
<li>Deploying perception models for advanced driving assistance systems (ADAS) or self-driving cars, where split-second decisions are critical.</li>
<li>Deploying ML models on machinery in remote fields with no internet connectivity, where immediate, on-the-spot results are required.</li>
</ul>
<h3 id="cloud-deployment">
  Cloud Deployment
  <a class="heading-link" href="#cloud-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Cloud deployment allows you to leverage the power and flexibility of cloud computing. This can be done in two main ways:</p>
<ol>
<li>
<p><strong>Self-Managed Cloud Deployments</strong>: Using tools like KFServing or KServe, organizations can deploy and manage ML models in the cloud on their own. This option provides extensive control but comes with a learning curve, making it more suitable for larger companies with the necessary resources.</p>
</li>
<li>
<p><strong>Managed Cloud Services</strong>: Alternatively, you can opt for managed services from major cloud providers like AWS, Azure, and Google Cloud Platform (GCP). These services simplify deployment and management, though they may have certain limitations.</p>
</li>
</ol>
<p>Below is a visual representation of the market share among major cloud providers for ML model deployment. The data is from 2022, and it&rsquo;s possible that Azure, with its partnership with OpenAI, has already surpassed AWS.</p>
<div style="text-align: center;">
    <img src="/images/cloud_providers.png" alt="Cloud Market Share" style="max-width: 50%; height: auto;" />
    
    <p style="text-align: center; font-style: italic; color: #666;">Cloud Market share @2022</p>
    
</div>

<h3 id="focus-on-aws-for-ml-deployment">
  Focus on AWS for ML Deployment
  <a class="heading-link" href="#focus-on-aws-for-ml-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>In this discussion, we&rsquo;ll focus on deploying machine learning models using AWS, although Azure and GCP offer similar services. Below are the key types of ML deployments available on AWS.</p>
<h4 id="aws-batch-processing">
  AWS Batch Processing
  <a class="heading-link" href="#aws-batch-processing">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>AWS Batch is a versatile batch processing service that enables you to run large-scale computing workloads on AWS.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Versatility:</strong> Suitable for a wide range of batch jobs, beyond just machine learning.</li>
<li><strong>Custom Docker Containers:</strong> Supports running jobs within custom Docker images, offering flexibility in the environment setup.</li>
<li><strong>Broad Use Cases:</strong> Applicable for tasks like data processing, image rendering, and large-scale simulations.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>General-Purpose Nature:</strong> Not specifically optimized for machine learning tasks, which might require additional setup or configuration.</li>
<li><strong>Potentially Complex Configuration:</strong> Depending on the use case, configuring batch jobs might require more detailed setup compared to services tailored to specific tasks like ML.</li>
</ul>
<h4 id="sagemaker-batch-transform">
  SageMaker Batch Transform
  <a class="heading-link" href="#sagemaker-batch-transform">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>SageMaker Batch Transform is designed specifically for batch inference tasks in machine learning.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>ML-Specific:</strong> Optimized for batch processing of machine learning model inferences, ensuring a streamlined workflow.</li>
<li><strong>Seamless Integration:</strong> Easily integrates with other SageMaker services like training and deployment, simplifying the end-to-end ML process.</li>
<li><strong>High Throughput:</strong> Designed for large-scale ML tasks, allowing for efficient processing of extensive datasets.</li>
<li><strong>Event-Driven:</strong> Can be triggered by S3 events, enabling automated processing of new data as it arrives.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Limited to ML Use Cases:</strong> Not suitable for non-ML batch processing tasks, which limits its flexibility compared to AWS Batch.</li>
<li><strong>Dependent on SageMaker Ecosystem:</strong> While integration is an advantage, it also means that it&rsquo;s less flexible if you need to use non-SageMaker services.</li>
</ul>
<h3 id="real-time-endpoints">
  Real-Time Endpoints
  <a class="heading-link" href="#real-time-endpoints">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Real-Time Endpoints in SageMaker are used for scenarios where immediate inference is required. They provide a scalable and low-latency way to get predictions as soon as data is sent to the model.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Low Latency:</strong> Delivers immediate responses, making it ideal for real-time applications.</li>
<li><strong>API Integration:</strong> Can be easily invoked via HTTP requests, making it straightforward to integrate into existing applications.</li>
<li><strong>Auto-Scaling:</strong> Automatically adjusts the number of instances based on demand, ensuring performance consistency.</li>
<li><strong>Flexible Instance Options:</strong> Offers both CPU and GPU instances, catering to different performance needs.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Payload Size Limit:</strong> The maximum input size is 6 MB, which might require you to downscale or compress larger inputs.</li>
<li><strong>Timeout Constraints:</strong> The model needs to return results within 60 seconds, which can be limiting for more complex models.</li>
</ul>
<h3 id="asynchronous-endpoints">
  Asynchronous Endpoints
  <a class="heading-link" href="#asynchronous-endpoints">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Asynchronous Endpoints are designed for scenarios where inference does not need to be instantaneous. They allow for processing larger payloads and longer processing times, making them suitable for more complex or batch-like tasks.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Handles Larger Payloads:</strong> Supports input sizes up to 1 GB, which is significantly higher than real-time endpoints.</li>
<li><strong>Extended Processing Time:</strong> Allows up to 15 minutes for inference, accommodating more complex computations.</li>
<li><strong>Queue Inputs:</strong> Inputs can be queued, which is useful for handling large volumes of data that do not require immediate processing.</li>
<li><strong>Auto-Scaling:</strong> Automatically scales based on demand, similar to real-time endpoints.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Near Real-Time Performance:</strong> While it supports larger payloads and longer processing times, the latency is higher compared to real-time endpoints.</li>
<li><strong>More Complex Workflow:</strong> Managing asynchronous jobs might require more careful monitoring and management compared to the simpler real-time endpoint approach.</li>
</ul>
<h3 id="sagemaker-serverless-inference">
  Sagemaker Serverless Inference
  <a class="heading-link" href="#sagemaker-serverless-inference">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Serverless Inference enables to deploy and scale ML models without configuring or managing any of the underlying infrastructure. We can use serveless inference for cpu only loads for requests where there is idle time between requests.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Autoscaling:</strong> Serverless Inference scales your endpoint down to 0, helping you to minimize your costs.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>CPU inference:</strong> Currently it supports only cpu compute.</li>
<li><strong>Cold start:</strong> We have to tolerate cold start for intial requests.</li>
</ul>
<p>More details here <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html"  class="external-link" target="_blank" rel="noopener">https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html</a></p>
<p>I will now go over throw show the code for Sagemaker Batch Transform, Real Time endpoints and Async Endpoints with the example of Instance segmentation using Detectron2 MaskRCNN implementation. Why instance segemntation example? I believe it is not covered as much as object detection and classification when it has the most utility in industry for computer vision tasks as it combine object detection with segmentation giving the class information along with precise object boundary for measurements. I will train the model using detectron2 example. Train it on sample dataset provided in detectron2 repo, the focus is not on the training but deployment but for the sake of completeness I will go over through complete example.</p>
<h2 id="instance-segmenation-training-and-inference-locally">
  Instance Segmenation Training and inference Locally
  <a class="heading-link" href="#instance-segmenation-training-and-inference-locally">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>An example of basic training and inference of detectron2 maskrcnn model is given in this repo <a href="https://github.com/facebookresearch/detectron2?tab=readme-ov-file"  class="external-link" target="_blank" rel="noopener">https://github.com/facebookresearch/detectron2?tab=readme-ov-file</a></p>
<p>A simple example of training maskrcnn model</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">detectron2.config</span> <span style="color:#ff7b72">import</span> get_cfg
</span></span><span style="display:flex;"><span>cfg <span style="color:#ff7b72;font-weight:bold">=</span> get_cfg()
</span></span><span style="display:flex;"><span>model_arch<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml&#34;</span>
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>merge_from_file(model_zoo<span style="color:#ff7b72;font-weight:bold">.</span>get_config_file(model_arch))
</span></span><span style="display:flex;"><span>trainer <span style="color:#ff7b72;font-weight:bold">=</span> DefaultTrainer(cfg) 
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff7b72;font-weight:bold">.</span>resume_or_load(resume<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#79c0ff">False</span>)
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff7b72;font-weight:bold">.</span>train()
</span></span></code></pre></div><p>and we can use this model to perorm inference like below</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">detectron2.config</span> <span style="color:#ff7b72">import</span> get_cfg
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">detectron2.engine</span> <span style="color:#ff7b72">import</span> DefaultPredictor
</span></span><span style="display:flex;"><span>cfg <span style="color:#ff7b72;font-weight:bold">=</span> get_cfg()
</span></span><span style="display:flex;"><span>model_arch<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml&#34;</span>
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>merge_from_file(model_zoo<span style="color:#ff7b72;font-weight:bold">.</span>get_config_file(model_arch))
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>MODEL<span style="color:#ff7b72;font-weight:bold">.</span>WEIGHTS <span style="color:#ff7b72;font-weight:bold">=</span> os<span style="color:#ff7b72;font-weight:bold">.</span>path<span style="color:#ff7b72;font-weight:bold">.</span>join(cfg<span style="color:#ff7b72;font-weight:bold">.</span>OUTPUT_DIR, <span style="color:#a5d6ff">&#34;model_final.pth&#34;</span>)  <span style="color:#8b949e;font-style:italic"># path to the model we just trained</span>
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>MODEL<span style="color:#ff7b72;font-weight:bold">.</span>ROI_HEADS<span style="color:#ff7b72;font-weight:bold">.</span>SCORE_THRESH_TEST <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">0.7</span>   <span style="color:#8b949e;font-style:italic"># set a custom testing threshold</span>
</span></span><span style="display:flex;"><span>predictor <span style="color:#ff7b72;font-weight:bold">=</span> DefaultPredictor(cfg)
</span></span><span style="display:flex;"><span>im <span style="color:#ff7b72;font-weight:bold">=</span> cv2<span style="color:#ff7b72;font-weight:bold">.</span>imread(d[<span style="color:#a5d6ff">&#34;file_name&#34;</span>])
</span></span><span style="display:flex;"><span>outputs <span style="color:#ff7b72;font-weight:bold">=</span> predictor(im)
</span></span></code></pre></div><p>see full example here <a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=U5LhISJqWXgM"  class="external-link" target="_blank" rel="noopener">https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=U5LhISJqWXgM</a></p>
<p>Once we have trained, evaluated and tested the model we want to deploy the model. We will show the most general case of deployment to build our own custom container so it is aplicable to pretty much all ML models not tied down to specific container proided by sagemaker. Complete code you can find <a href="https://github.com/Asad-Ismail/ml-model-deployment"  class="external-link" target="_blank" rel="noopener">Github Repo</a></p>
<h2 id="build-custom-cotainer">
  Build Custom Cotainer
  <a class="heading-link" href="#build-custom-cotainer">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Before deploying our models to any of the deployment strategy we need to first dockerize our model. Below is the docker file for async and real time endpoints</p>
<pre tabindex="0"><code>FROM nvcr.io/nvidia/pytorch:23.12-py3 as build

RUN apt update &amp;&amp; DEBIAN_FRONTEND=&#34;noninteractive&#34; apt -y install tzdata &amp;&amp; \
    apt install -y zip libgl1-mesa-glx netbase libopencv-dev libopenblas-dev nginx &amp;&amp; \
    apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py &amp;&amp; \
    python get-pip.py &amp;&amp; \
    python -m pip install pip==24.2 --no-cache-dir

COPY ./scripts /opt/program

# Set work directory and install Python dependencies
WORKDIR /opt/program
## specific torch and torchvision version
RUN pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113 --no-cache-dir

RUN pip install -r requirements.txt --no-cache-dir

# Set environment variables
ENV PYTHONUNBUFFERED=TRUE \
    PYTHONDONTWRITEBYTECODE=TRUE \
    PATH=&#34;/opt/program:${PATH}&#34;

RUN chmod +x /opt/program/serve
</code></pre><p>To build docker and push to ECR</p>
<pre tabindex="0"><code>docker build -t $image_tag -f $dockerfile .
docker tag $image_tag $account.dkr.ecr.$region.amazonaws.com/$repo_name:latest
docker push $account.dkr.ecr.$region.amazonaws.com/$repo_name:latest
</code></pre><p>It essentialy gets base nvidia image from nvcr and install dependecies for running detectron2.</p>
<h2 id="regestring-model">
  Regestring Model
  <a class="heading-link" href="#regestring-model">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>To properly arrage and register the models and have proper meta data assigned to them it is better to arrage models in Model groups. We can also skip this step and go dorectly to next step but I highly recommend it. Model at this point is defined by two things a model container which was pushed to ECR in the last step and model gzipped file which contains model config file and model weights, using both we can deploy the model in sagemaker.</p>
<pre tabindex="0"><code>
model_package_group_name = &#34;name-of-model-group&#34;
model_package_group_input_dict = {
 &#34;ModelPackageGroupName&#34; : model_package_group_name,
 &#34;ModelPackageGroupDescription&#34; : &#34;Test Sample model package group&#34;
}
create_model_package_group_response = sm_client.create_model_package_group(**model_package_group_input_dict)

modelpackage_inference_specification =  {
    &#34;InferenceSpecification&#34;: {
      &#34;Containers&#34;: [
         {
            &#34;Image&#34;: image_uri,
            &#34;ModelDataUrl&#34;: model_url
         }
      ],
      &#34;SupportedContentTypes&#34;: [ &#34;json&#34; ],
      &#34;SupportedResponseMIMETypes&#34;: [ &#34;json/csv&#34; ],
   }
 }

 create_model_package_input_dict = {
    &#34;ModelPackageGroupName&#34; : model_package_group_name,
    &#34;ModelPackageDescription&#34; : &#34;Model to do something x&#34;,
    &#34;ModelApprovalStatus&#34; : &#34;PendingManualApproval&#34;
}
create_model_package_input_dict.update(modelpackage_inference_specification)
create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)
model_package_arn = create_model_package_response[&#34;ModelPackageArn&#34;]
</code></pre><h2 id="real-time-endpoints-1">
  Real Time endpoints
  <a class="heading-link" href="#real-time-endpoints-1">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<pre tabindex="0"><code>
# Create a SageMaker session and create model group
sagemaker_session = sagemaker.Session()
model_package_arn=&#34;arn:aws:sagemaker:xxxx&#34;

model = ModelPackage(role=role, 
                     model_package_arn=model_package_arn, 
                     sagemaker_session=sagemaker_session)

async_config = AsyncInferenceConfig(
    output_path=&#39;s3://your-bucket/async_output/&#39;,
    max_concurrent_invocations_per_instance=1
)

# Deploy the model to an endpoint
predictor = model.deploy(
    initial_instance_count=1,
    instance_type=&#34;ml.p3.2xlarge&#34;,
    endpoint_name=&#34;your-endpoint-name&#34;
)

# Perform real-time inference
test_image = open(&#34;test_image.jpg&#34;, &#34;rb&#34;).read()
response = predictor.predict(test_image)
</code></pre><h2 id="aync-inference">
  Aync Inference
  <a class="heading-link" href="#aync-inference">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<pre tabindex="0"><code>
# Create a SageMaker session and specify your role
sagemaker_session = sagemaker.Session()
model_package_arn=&#34;arn:aws:sagemaker:xxxx&#34;

model = ModelPackage(role=role, 
                     model_package_arn=model_package_arn, 
                     sagemaker_session=sagemaker_session)

async_config = AsyncInferenceConfig(
    output_path=&#39;s3://your-bucket/async_output/&#39;,
    max_concurrent_invocations_per_instance=1
)

# Deploy model as endpoint
predictor = model.deploy(
    initial_instance_count=1,
    instance_type=&#39;ml.p3.2xlarge&#39;,
    endpoint_name=endpoint_name,
    async_inference_config=async_config
)

waiter = sm_client.get_waiter(&#34;endpoint_in_service&#34;)
print(&#34;Waiting for endpoint to create...&#34;)
waiter.wait(EndpointName=endpoint_name)
resp = sm_client.describe_endpoint(EndpointName=endpoint_name)
</code></pre><h3 id="attaching-autoscaling-to-endpoint">
  Attaching Autoscaling to endpoint
  <a class="heading-link" href="#attaching-autoscaling-to-endpoint">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>If we expect irregular requests to save cost and be responsive, it is best to attach autoscaling to the endpoints so it can scale out if we get lot of traffic requests and scale in if there is not much requests. We can have different metrics to montior to scale out and in here we are using SageMakerVariantInvocationsPerInstance.</p>
<pre tabindex="0"><code>
client = boto3.client(&#34;application-autoscaling&#34;) 

## Using autoscaling for all trafic and the current endpoint
resource_id = (&#34;endpoint/&#34; + endpoint_name + &#34;/variant/&#34; + &#34;AllTraffic&#34;)  

# Configure Autoscaling on asynchronous endpoint down to zero instances
response = client.register_scalable_target(
    ServiceNamespace=&#34;sagemaker&#34;,
    ResourceId=resource_id,
    ScalableDimension=&#34;sagemaker:variant:DesiredInstanceCount&#34;,
    MinCapacity=0,
    MaxCapacity=1,
)

response = client.put_scaling_policy(
    PolicyName=&#34;Invocations-ScalingPolicy&#34;,
    ServiceNamespace=&#34;sagemaker&#34;,  # The namespace of the AWS service that provides the resource.
    ResourceId=resource_id,  # Endpoint name
    ScalableDimension=&#34;sagemaker:variant:DesiredInstanceCount&#34;,  # SageMaker supports only Instance Count
    PolicyType=&#34;TargetTrackingScaling&#34;,  # &#39;StepScaling&#39;|&#39;TargetTrackingScaling&#39;
    TargetTrackingScalingPolicyConfiguration={
        &#34;TargetValue&#34;: 1.0,  # The target value for the metric. - here the metric is - SageMakerVariantInvocationsPerInstance
        &#34;PredefinedMetricSpecification&#34;: {
            &#34;PredefinedMetricType&#34;: &#34;SageMakerVariantInvocationsPerInstance&#34;
        },
        &#34;ScaleInCooldown&#34;: 2,  # The cooldown period helps you prevent your Auto Scaling group from launching or terminating
        # additional instances before the effects of previous activities are visible.
        # You can configure the length of time based on your instance startup time or other application needs.
        # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start.
        &#34;ScaleOutCooldown&#34;: 2,  # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.
        # &#39;DisableScaleIn&#39;: True|False - ndicates whether scale in by the target tracking policy is disabled.
        # If the value is true , scale in is disabled and the target tracking policy won&#39;t remove capacity from the scalable resource.
        &#34;DisableScaleIn&#34;: False 
    },
)
</code></pre><h3 id="aws-sagemaker-batch-transform">
  AWS Sagemaker Batch Transform
  <a class="heading-link" href="#aws-sagemaker-batch-transform">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>
<p>For sagemaker batch transform the docker file is very slightly different see detail here <a href="https://github.com/Asad-Ismail/ml-model-deployment/blob/main/sm_batch_transform/Dockerfile"  class="external-link" target="_blank" rel="noopener">Doclerfile_BT</a></p>
</li>
<li>
<p>Building and registering the model steps stays the same also given her <a href="https://github.com/Asad-Ismail/ml-model-deployment/blob/main/sm_batch_transform/Dockerfile"  class="external-link" target="_blank" rel="noopener">Build&amp;Register</a></p>
</li>
<li>
<p>Create sagemaker batch transform job</p>
</li>
</ol>
<pre tabindex="0"><code>
sm_cli = sagemaker_session.sagemaker_client

transformer=model.transformer(instance_count=1, instance_type=&#34;ml.p3.2xlarge&#34;, output_path=model_outputs_path,max_payload=100)

transformer.transform(
    data=&#39;s3://path_to_your_images/input/&#39;, 
    data_type=&#34;S3Prefix&#34;,
    content_type=&#34;application/x-image&#34;,
    wait=True
)

job_info = sm_cli.describe_transform_job(TransformJobName=transformer.latest_transform_job.name)
</code></pre>
      </div>


      <footer>
        


        
        
        
        
        

        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  
  



  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
