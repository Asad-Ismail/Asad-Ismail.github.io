<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>
  Machine Learning Model deployment · Asad Ismail
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Asad Ismail">
<meta name="description" content="Machine learning deployment">
<meta name="keywords" content="blog,developer,personal">
<meta name="fediverse:creator" content="" />


  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Machine Learning Model deployment">
  <meta name="twitter:description" content="Machine learning deployment">

<meta property="og:url" content="http://localhost:1313/posts/ml_deployment/">
  <meta property="og:site_name" content="Asad Ismail">
  <meta property="og:title" content="Machine Learning Model deployment">
  <meta property="og:description" content="Machine learning deployment">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-16T00:36:50+02:00">
    <meta property="article:modified_time" content="2024-08-16T00:36:50+02:00">




<link rel="canonical" href="http://localhost:1313/posts/ml_deployment/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.css" media="screen">






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.css" media="screen">
  



 




<link rel="icon" type="image/svg+xml" href="/img/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/img/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/img/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://localhost:1313/">
      Asad Ismail
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/">Home</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/projects/">Projects</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://localhost:1313/posts/ml_deployment/">
              Machine Learning Model deployment
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2024-08-16T00:36:50&#43;02:00">
                August 16, 2024
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              11-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        <h2 id="machine-learning-model-deployment">
  Machine Learning Model Deployment
  <a class="heading-link" href="#machine-learning-model-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>After successfully building your ML/DL model in a notebook, the next critical step is deployment—getting your model out into the world to serve customers. There are two primary avenues for deploying machine learning models:</p>
<ol>
<li><strong>Edge Deployment</strong></li>
<li><strong>Cloud Deployment</strong></li>
</ol>
<h3 id="edge-deployment">
  Edge Deployment
  <a class="heading-link" href="#edge-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Edge deployment involves placing the model directly on the device where it will be used. This approach is particularly valuable in situations where:</p>
<ol>
<li><strong>Latency Requirements</strong>: Real-time deployment and control are essential, and network latency is unacceptably high.</li>
<li><strong>No Network Availability</strong>: The model must operate in environments without internet connectivity.</li>
</ol>
<h4 id="examples">
  Examples:
  <a class="heading-link" href="#examples">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<ul>
<li>Deploying perception models for advanced driving assistance systems (ADAS) or self-driving cars, where split-second decisions are critical.</li>
<li>Deploying ML models on machinery in remote fields with no internet connectivity, where immediate, on-the-spot results are required.</li>
</ul>
<h3 id="cloud-deployment">
  Cloud Deployment
  <a class="heading-link" href="#cloud-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Cloud deployment allows you to leverage the power and flexibility of cloud computing. This can be done in two main ways:</p>
<ol>
<li>
<p><strong>Self-Managed Cloud Deployments</strong>: Using tools like KFServing or KServe, organizations can deploy and manage ML models in the cloud on their own. This option provides extensive control but comes with a learning curve, making it more suitable for larger companies with the necessary resources.</p>
</li>
<li>
<p><strong>Managed Cloud Services</strong>: Alternatively, you can opt for managed services from major cloud providers like AWS, Azure, and Google Cloud Platform (GCP). These services simplify deployment and management, though they may have certain limitations.</p>
</li>
</ol>
<p>Below is a visual representation of the market share among major cloud providers for ML model deployment. The data is from 2022, and it&rsquo;s possible that Azure, with its partnership with OpenAI, has already surpassed AWS.</p>
<div style="text-align: center;">
    <img src="/images/cloud_providers.png" alt="Cloud Market Share" style="max-width: 50%; height: auto;" />
    
    <p style="text-align: center; font-style: italic; color: #666;">Cloud Market share @2022</p>
    
</div>

<h3 id="focus-on-aws-for-ml-deployment">
  Focus on AWS for ML Deployment
  <a class="heading-link" href="#focus-on-aws-for-ml-deployment">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>In this discussion, we&rsquo;ll focus on deploying machine learning models using AWS, although Azure and GCP offer similar services. Below are the key types of ML deployments available on AWS.</p>
<h4 id="aws-batch-processing">
  AWS Batch Processing
  <a class="heading-link" href="#aws-batch-processing">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>AWS Batch is a versatile batch processing service that enables you to run large-scale computing workloads on AWS.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Versatility:</strong> Suitable for a wide range of batch jobs, beyond just machine learning.</li>
<li><strong>Custom Docker Containers:</strong> Supports running jobs within custom Docker images, offering flexibility in the environment setup.</li>
<li><strong>Broad Use Cases:</strong> Applicable for tasks like data processing, image rendering, and large-scale simulations.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>General-Purpose Nature:</strong> Not specifically optimized for machine learning tasks, which might require additional setup or configuration.</li>
<li><strong>Potentially Complex Configuration:</strong> Depending on the use case, configuring batch jobs might require more detailed setup compared to services tailored to specific tasks like ML.</li>
</ul>
<h4 id="sagemaker-batch-transform">
  SageMaker Batch Transform
  <a class="heading-link" href="#sagemaker-batch-transform">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>SageMaker Batch Transform is designed specifically for batch inference tasks in machine learning.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>ML-Specific:</strong> Optimized for batch processing of machine learning model inferences, ensuring a streamlined workflow.</li>
<li><strong>Seamless Integration:</strong> Easily integrates with other SageMaker services like training and deployment, simplifying the end-to-end ML process.</li>
<li><strong>High Throughput:</strong> Designed for large-scale ML tasks, allowing for efficient processing of extensive datasets.</li>
<li><strong>Event-Driven:</strong> Can be triggered by S3 events, enabling automated processing of new data as it arrives.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Limited to ML Use Cases:</strong> Not suitable for non-ML batch processing tasks, which limits its flexibility compared to AWS Batch.</li>
<li><strong>Dependent on SageMaker Ecosystem:</strong> While integration is an advantage, it also means that it&rsquo;s less flexible if you need to use non-SageMaker services.</li>
</ul>
<h3 id="real-time-endpoints">
  Real-Time Endpoints
  <a class="heading-link" href="#real-time-endpoints">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Real-Time Endpoints in SageMaker are used for scenarios where immediate inference is required. They provide a scalable and low-latency way to get predictions as soon as data is sent to the model.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Low Latency:</strong> Delivers immediate responses, making it ideal for real-time applications.</li>
<li><strong>API Integration:</strong> Can be easily invoked via HTTP requests, making it straightforward to integrate into existing applications.</li>
<li><strong>Auto-Scaling:</strong> Automatically adjusts the number of instances based on demand, ensuring performance consistency.</li>
<li><strong>Flexible Instance Options:</strong> Offers both CPU and GPU instances, catering to different performance needs.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Payload Size Limit:</strong> The maximum input size is 6 MB, which might require you to downscale or compress larger inputs.</li>
<li><strong>Timeout Constraints:</strong> The model needs to return results within 60 seconds, which can be limiting for more complex models.</li>
</ul>
<h3 id="asynchronous-endpoints">
  Asynchronous Endpoints
  <a class="heading-link" href="#asynchronous-endpoints">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Asynchronous Endpoints are designed for scenarios where inference does not need to be instantaneous. They allow for processing larger payloads and longer processing times, making them suitable for more complex or batch-like tasks.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Handles Larger Payloads:</strong> Supports input sizes up to 1 GB, which is significantly higher than real-time endpoints.</li>
<li><strong>Extended Processing Time:</strong> Allows up to 15 minutes for inference, accommodating more complex computations.</li>
<li><strong>Queue Inputs:</strong> Inputs can be queued, which is useful for handling large volumes of data that do not require immediate processing.</li>
<li><strong>Auto-Scaling:</strong> Automatically scales based on demand, similar to real-time endpoints.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Near Real-Time Performance:</strong> While it supports larger payloads and longer processing times, the latency is higher compared to real-time endpoints.</li>
<li><strong>More Complex Workflow:</strong> Managing asynchronous jobs might require more careful monitoring and management compared to the simpler real-time endpoint approach.</li>
</ul>
<h3 id="sagemaker-serverless-inference">
  Sagemaker Serverless Inference
  <a class="heading-link" href="#sagemaker-serverless-inference">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Serverless Inference enables to deploy and scale ML models without configuring or managing any of the underlying infrastructure. We can use serveless inference for cpu only loads for requests where there is idle time between requests.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Autoscaling:</strong> Serverless Inference scales your endpoint down to 0, helping you to minimize your costs.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>CPU inference:</strong> Currently it supports only cpu compute.</li>
<li><strong>Cold start:</strong> We have to tolerate cold start for intial requests.</li>
</ul>
<p>More details here <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html"  class="external-link" target="_blank" rel="noopener">https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html</a></p>
<p>I will now go through and show the code for SageMaker Batch Transform, Real-Time Endpoints, and Async Endpoints with the example of instance segmentation using Detectron2&rsquo;s Mask R-CNN implementation. Why an instance segmentation example? I believe it is not covered as much as object detection and classification, even though it has the most utility in the industry for computer vision tasks, as it combines object detection with segmentation, giving the class information along with precise object boundaries for measurements. I will train the model using a Detectron2 example, and train it on a sample dataset provided in the Detectron2 repo. The focus is not on the training but deployment, but for the sake of completeness, I will go through the complete example.</p>
<h2 id="instance-segmentation-training-and-inference-locally">
  Instance Segmentation Training and inference Locally
  <a class="heading-link" href="#instance-segmentation-training-and-inference-locally">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>An example of basic training and inference of detectron2 maskrcnn model is given in this repo <a href="https://github.com/facebookresearch/detectron2?tab=readme-ov-file"  class="external-link" target="_blank" rel="noopener">Detectron2</a></p>
<p>A simple example of training maskrcnn model</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">detectron2.config</span> <span style="color:#ff7b72">import</span> get_cfg
</span></span><span style="display:flex;"><span>cfg <span style="color:#ff7b72;font-weight:bold">=</span> get_cfg()
</span></span><span style="display:flex;"><span>model_arch<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml&#34;</span>
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>merge_from_file(model_zoo<span style="color:#ff7b72;font-weight:bold">.</span>get_config_file(model_arch))
</span></span><span style="display:flex;"><span>trainer <span style="color:#ff7b72;font-weight:bold">=</span> DefaultTrainer(cfg) 
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff7b72;font-weight:bold">.</span>resume_or_load(resume<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#79c0ff">False</span>)
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff7b72;font-weight:bold">.</span>train()
</span></span></code></pre></div><p>and we can use this model to perorm inference like below</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">detectron2.config</span> <span style="color:#ff7b72">import</span> get_cfg
</span></span><span style="display:flex;"><span><span style="color:#ff7b72">from</span> <span style="color:#ff7b72">detectron2.engine</span> <span style="color:#ff7b72">import</span> DefaultPredictor
</span></span><span style="display:flex;"><span>cfg <span style="color:#ff7b72;font-weight:bold">=</span> get_cfg()
</span></span><span style="display:flex;"><span>model_arch<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;COCO-InstanceSegmentation/mask_rcnn_R_101_C4_3x.yaml&#34;</span>
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>merge_from_file(model_zoo<span style="color:#ff7b72;font-weight:bold">.</span>get_config_file(model_arch))
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>MODEL<span style="color:#ff7b72;font-weight:bold">.</span>WEIGHTS <span style="color:#ff7b72;font-weight:bold">=</span> os<span style="color:#ff7b72;font-weight:bold">.</span>path<span style="color:#ff7b72;font-weight:bold">.</span>join(cfg<span style="color:#ff7b72;font-weight:bold">.</span>OUTPUT_DIR, <span style="color:#a5d6ff">&#34;model_final.pth&#34;</span>)  <span style="color:#8b949e;font-style:italic"># path to the model we just trained</span>
</span></span><span style="display:flex;"><span>cfg<span style="color:#ff7b72;font-weight:bold">.</span>MODEL<span style="color:#ff7b72;font-weight:bold">.</span>ROI_HEADS<span style="color:#ff7b72;font-weight:bold">.</span>SCORE_THRESH_TEST <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">0.7</span>   <span style="color:#8b949e;font-style:italic"># set a custom testing threshold</span>
</span></span><span style="display:flex;"><span>predictor <span style="color:#ff7b72;font-weight:bold">=</span> DefaultPredictor(cfg)
</span></span><span style="display:flex;"><span>im <span style="color:#ff7b72;font-weight:bold">=</span> cv2<span style="color:#ff7b72;font-weight:bold">.</span>imread(d[<span style="color:#a5d6ff">&#34;file_name&#34;</span>])
</span></span><span style="display:flex;"><span>outputs <span style="color:#ff7b72;font-weight:bold">=</span> predictor(im)
</span></span></code></pre></div><p>see full example here <a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=U5LhISJqWXgM"  class="external-link" target="_blank" rel="noopener">Detectron2 Example</a></p>
<p>Once we have trained, evaluated, and tested the model, we want to deploy it. We will show the most general case of deployment by building our own custom container so it is applicable to pretty much all ML models, not tied down to specific containers provided by SageMaker. The complete code can be found in the <a href="https://github.com/Asad-Ismail/ml-model-deployment"  class="external-link" target="_blank" rel="noopener">Repo</a></p>
<h2 id="build-custom-container">
  Build Custom Container
  <a class="heading-link" href="#build-custom-container">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Before deploying our models to any of the deployment strategies, we need to first dockerize our model. Below is the Dockerfile for Async and Real-Time Endpoints:</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>FROM nvcr.io/nvidia/pytorch:23.12-py3 as build
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>RUN apt update <span style="color:#ff7b72;font-weight:bold">&amp;&amp;</span> <span style="color:#79c0ff">DEBIAN_FRONTEND</span><span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;noninteractive&#34;</span> apt -y install tzdata <span style="color:#ff7b72;font-weight:bold">&amp;&amp;</span> <span style="color:#79c0ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#79c0ff"></span>    apt install -y zip libgl1-mesa-glx netbase libopencv-dev libopenblas-dev nginx <span style="color:#ff7b72;font-weight:bold">&amp;&amp;</span> <span style="color:#79c0ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#79c0ff"></span>    apt clean <span style="color:#ff7b72;font-weight:bold">&amp;&amp;</span> rm -rf /var/lib/apt/lists/*
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py <span style="color:#ff7b72;font-weight:bold">&amp;&amp;</span> <span style="color:#79c0ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#79c0ff"></span>    python get-pip.py <span style="color:#ff7b72;font-weight:bold">&amp;&amp;</span> <span style="color:#79c0ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#79c0ff"></span>    python -m pip install <span style="color:#79c0ff">pip</span><span style="color:#ff7b72;font-weight:bold">==</span>24.2 --no-cache-dir
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>COPY ./scripts /opt/program
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># Set work directory and install Python dependencies</span>
</span></span><span style="display:flex;"><span>WORKDIR /opt/program
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic">## specific torch and torchvision version</span>
</span></span><span style="display:flex;"><span>RUN pip install <span style="color:#79c0ff">torch</span><span style="color:#ff7b72;font-weight:bold">==</span>1.12.1+cu113 <span style="color:#79c0ff">torchvision</span><span style="color:#ff7b72;font-weight:bold">==</span>0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113 --no-cache-dir
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>RUN pip install -r requirements.txt --no-cache-dir
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># Set environment variables</span>
</span></span><span style="display:flex;"><span>ENV <span style="color:#79c0ff">PYTHONUNBUFFERED</span><span style="color:#ff7b72;font-weight:bold">=</span>TRUE <span style="color:#79c0ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#79c0ff"></span>    <span style="color:#79c0ff">PYTHONDONTWRITEBYTECODE</span><span style="color:#ff7b72;font-weight:bold">=</span>TRUE <span style="color:#79c0ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#79c0ff"></span>    <span style="color:#79c0ff">PATH</span><span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;/opt/program:</span><span style="color:#a5d6ff">${</span><span style="color:#79c0ff">PATH</span><span style="color:#a5d6ff">}</span><span style="color:#a5d6ff">&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>RUN chmod +x /opt/program/serve
</span></span></code></pre></div><p>To build docker and push to ECR</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker build -t <span style="color:#79c0ff">$image_tag</span> -f <span style="color:#79c0ff">$dockerfile</span> .
</span></span><span style="display:flex;"><span>docker tag <span style="color:#79c0ff">$image_tag</span> <span style="color:#79c0ff">$account</span>.dkr.ecr.<span style="color:#79c0ff">$region</span>.amazonaws.com/<span style="color:#79c0ff">$repo_name</span>:latest
</span></span><span style="display:flex;"><span>docker push <span style="color:#79c0ff">$account</span>.dkr.ecr.<span style="color:#79c0ff">$region</span>.amazonaws.com/<span style="color:#79c0ff">$repo_name</span>:latest
</span></span></code></pre></div><p>It essentially gets the base NVIDIA image from NGC and installs dependencies for running Detectron2.</p>
<h2 id="registering-the-model">
  Registering the Model
  <a class="heading-link" href="#registering-the-model">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>To properly arrange and register the models and have proper metadata assigned to them, it is better to arrange models in Model Groups. We can also skip this step and go directly to the next step, but I highly recommend it. The model at this point is defined by two things: a model container, which was pushed to ECR in the last step, and a gzipped model file, which contains the model config file and model weights. Using both, we can deploy the model in SageMaker.</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_package_group_name <span style="color:#ff7b72;font-weight:bold">=</span> <span style="color:#a5d6ff">&#34;name-of-model-group&#34;</span>
</span></span><span style="display:flex;"><span>model_package_group_input_dict <span style="color:#ff7b72;font-weight:bold">=</span> {
</span></span><span style="display:flex;"><span> <span style="color:#a5d6ff">&#34;ModelPackageGroupName&#34;</span> : model_package_group_name,
</span></span><span style="display:flex;"><span> <span style="color:#a5d6ff">&#34;ModelPackageGroupDescription&#34;</span> : <span style="color:#a5d6ff">&#34;Test Sample model package group&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>create_model_package_group_response <span style="color:#ff7b72;font-weight:bold">=</span> sm_client<span style="color:#ff7b72;font-weight:bold">.</span>create_model_package_group(<span style="color:#ff7b72;font-weight:bold">**</span>model_package_group_input_dict)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>modelpackage_inference_specification <span style="color:#ff7b72;font-weight:bold">=</span>  {
</span></span><span style="display:flex;"><span>    <span style="color:#a5d6ff">&#34;InferenceSpecification&#34;</span>: {
</span></span><span style="display:flex;"><span>      <span style="color:#a5d6ff">&#34;Containers&#34;</span>: [
</span></span><span style="display:flex;"><span>         {
</span></span><span style="display:flex;"><span>            <span style="color:#a5d6ff">&#34;Image&#34;</span>: image_uri,
</span></span><span style="display:flex;"><span>            <span style="color:#a5d6ff">&#34;ModelDataUrl&#34;</span>: model_url
</span></span><span style="display:flex;"><span>         }
</span></span><span style="display:flex;"><span>      ],
</span></span><span style="display:flex;"><span>      <span style="color:#a5d6ff">&#34;SupportedContentTypes&#34;</span>: [ <span style="color:#a5d6ff">&#34;json&#34;</span> ],
</span></span><span style="display:flex;"><span>      <span style="color:#a5d6ff">&#34;SupportedResponseMIMETypes&#34;</span>: [ <span style="color:#a5d6ff">&#34;json/csv&#34;</span> ],
</span></span><span style="display:flex;"><span>   }
</span></span><span style="display:flex;"><span> }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> create_model_package_input_dict <span style="color:#ff7b72;font-weight:bold">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#a5d6ff">&#34;ModelPackageGroupName&#34;</span> : model_package_group_name,
</span></span><span style="display:flex;"><span>    <span style="color:#a5d6ff">&#34;ModelPackageDescription&#34;</span> : <span style="color:#a5d6ff">&#34;Model to do something x&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#a5d6ff">&#34;ModelApprovalStatus&#34;</span> : <span style="color:#a5d6ff">&#34;PendingManualApproval&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>create_model_package_input_dict<span style="color:#ff7b72;font-weight:bold">.</span>update(modelpackage_inference_specification)
</span></span><span style="display:flex;"><span>create_model_package_response <span style="color:#ff7b72;font-weight:bold">=</span> sm_client<span style="color:#ff7b72;font-weight:bold">.</span>create_model_package(<span style="color:#ff7b72;font-weight:bold">**</span>create_model_package_input_dict)
</span></span><span style="display:flex;"><span>model_package_arn <span style="color:#ff7b72;font-weight:bold">=</span> create_model_package_response[<span style="color:#a5d6ff">&#34;ModelPackageArn&#34;</span>]
</span></span></code></pre></div><h2 id="real-time-endpoints-1">
  Real Time endpoints
  <a class="heading-link" href="#real-time-endpoints-1">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>To deploy our model as a real-time endpoint using SageMaker, we can use the ModelPackage class to create a model from our registered model package and then deploy it to an endpoint. This setup allows you to serve predictions in real time with minimal latency, which is ideal for applications requiring immediate responses.</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># Create a SageMaker session and create model group</span>
</span></span><span style="display:flex;"><span>sagemaker_session <span style="color:#ff7b72;font-weight:bold">=</span> sagemaker<span style="color:#ff7b72;font-weight:bold">.</span>Session()
</span></span><span style="display:flex;"><span>model_package_arn<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;arn:aws:sagemaker:xxxx&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#ff7b72;font-weight:bold">=</span> ModelPackage(role<span style="color:#ff7b72;font-weight:bold">=</span>role, 
</span></span><span style="display:flex;"><span>                     model_package_arn<span style="color:#ff7b72;font-weight:bold">=</span>model_package_arn, 
</span></span><span style="display:flex;"><span>                     sagemaker_session<span style="color:#ff7b72;font-weight:bold">=</span>sagemaker_session)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>async_config <span style="color:#ff7b72;font-weight:bold">=</span> AsyncInferenceConfig(
</span></span><span style="display:flex;"><span>    output_path<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#39;s3://your-bucket/async_output/&#39;</span>,
</span></span><span style="display:flex;"><span>    max_concurrent_invocations_per_instance<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># Deploy the model to an endpoint</span>
</span></span><span style="display:flex;"><span>predictor <span style="color:#ff7b72;font-weight:bold">=</span> model<span style="color:#ff7b72;font-weight:bold">.</span>deploy(
</span></span><span style="display:flex;"><span>    initial_instance_count<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1</span>,
</span></span><span style="display:flex;"><span>    instance_type<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;ml.p3.2xlarge&#34;</span>,
</span></span><span style="display:flex;"><span>    endpoint_name<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;your-endpoint-name&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># Perform real-time inference</span>
</span></span><span style="display:flex;"><span>test_image <span style="color:#ff7b72;font-weight:bold">=</span> open(<span style="color:#a5d6ff">&#34;test_image.jpg&#34;</span>, <span style="color:#a5d6ff">&#34;rb&#34;</span>)<span style="color:#ff7b72;font-weight:bold">.</span>read()
</span></span><span style="display:flex;"><span>response <span style="color:#ff7b72;font-weight:bold">=</span> predictor<span style="color:#ff7b72;font-weight:bold">.</span>predict(test_image)
</span></span></code></pre></div><h2 id="aync-inference">
  Aync Inference
  <a class="heading-link" href="#aync-inference">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>For scenarios where immediate responses are not critical and you need to handle large payloads or longer processing times, you can deploy your model using SageMaker&rsquo;s Asynchronous Inference. This method allows your endpoint to process requests asynchronously, making it suitable for batch processing or complex computations.</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># Create a SageMaker session and specify your role</span>
</span></span><span style="display:flex;"><span>sagemaker_session <span style="color:#ff7b72;font-weight:bold">=</span> sagemaker<span style="color:#ff7b72;font-weight:bold">.</span>Session()
</span></span><span style="display:flex;"><span>model_package_arn<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;arn:aws:sagemaker:xxxx&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#ff7b72;font-weight:bold">=</span> ModelPackage(role<span style="color:#ff7b72;font-weight:bold">=</span>role, 
</span></span><span style="display:flex;"><span>                     model_package_arn<span style="color:#ff7b72;font-weight:bold">=</span>model_package_arn, 
</span></span><span style="display:flex;"><span>                     sagemaker_session<span style="color:#ff7b72;font-weight:bold">=</span>sagemaker_session)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>async_config <span style="color:#ff7b72;font-weight:bold">=</span> AsyncInferenceConfig(
</span></span><span style="display:flex;"><span>    output_path<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#39;s3://your-bucket/async_output/&#39;</span>,
</span></span><span style="display:flex;"><span>    max_concurrent_invocations_per_instance<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># Deploy model as endpoint</span>
</span></span><span style="display:flex;"><span>predictor <span style="color:#ff7b72;font-weight:bold">=</span> model<span style="color:#ff7b72;font-weight:bold">.</span>deploy(
</span></span><span style="display:flex;"><span>    initial_instance_count<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1</span>,
</span></span><span style="display:flex;"><span>    instance_type<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#39;ml.p3.2xlarge&#39;</span>,
</span></span><span style="display:flex;"><span>    endpoint_name<span style="color:#ff7b72;font-weight:bold">=</span>endpoint_name,
</span></span><span style="display:flex;"><span>    async_inference_config<span style="color:#ff7b72;font-weight:bold">=</span>async_config
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>waiter <span style="color:#ff7b72;font-weight:bold">=</span> sm_client<span style="color:#ff7b72;font-weight:bold">.</span>get_waiter(<span style="color:#a5d6ff">&#34;endpoint_in_service&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#a5d6ff">&#34;Waiting for endpoint to create...&#34;</span>)
</span></span><span style="display:flex;"><span>waiter<span style="color:#ff7b72;font-weight:bold">.</span>wait(EndpointName<span style="color:#ff7b72;font-weight:bold">=</span>endpoint_name)
</span></span><span style="display:flex;"><span>resp <span style="color:#ff7b72;font-weight:bold">=</span> sm_client<span style="color:#ff7b72;font-weight:bold">.</span>describe_endpoint(EndpointName<span style="color:#ff7b72;font-weight:bold">=</span>endpoint_name)
</span></span></code></pre></div><h3 id="attaching-autoscaling-to-endpoint">
  Attaching Autoscaling to endpoint
  <a class="heading-link" href="#attaching-autoscaling-to-endpoint">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>If we expect irregular requests, to save cost and be responsive, it is best to attach autoscaling to the endpoints so it can scale out if we get a lot of traffic requests and scale in if there are not many requests. We can have different metrics to monitor to scale out and in; here we are using SageMakerVariantInvocationsPerInstance.</p>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>client <span style="color:#ff7b72;font-weight:bold">=</span> boto3<span style="color:#ff7b72;font-weight:bold">.</span>client(<span style="color:#a5d6ff">&#34;application-autoscaling&#34;</span>) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic">## Using autoscaling for all trafic and the current endpoint</span>
</span></span><span style="display:flex;"><span>resource_id <span style="color:#ff7b72;font-weight:bold">=</span> (<span style="color:#a5d6ff">&#34;endpoint/&#34;</span> <span style="color:#ff7b72;font-weight:bold">+</span> endpoint_name <span style="color:#ff7b72;font-weight:bold">+</span> <span style="color:#a5d6ff">&#34;/variant/&#34;</span> <span style="color:#ff7b72;font-weight:bold">+</span> <span style="color:#a5d6ff">&#34;AllTraffic&#34;</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8b949e;font-style:italic"># Configure Autoscaling on asynchronous endpoint down to zero instances</span>
</span></span><span style="display:flex;"><span>response <span style="color:#ff7b72;font-weight:bold">=</span> client<span style="color:#ff7b72;font-weight:bold">.</span>register_scalable_target(
</span></span><span style="display:flex;"><span>    ServiceNamespace<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;sagemaker&#34;</span>,
</span></span><span style="display:flex;"><span>    ResourceId<span style="color:#ff7b72;font-weight:bold">=</span>resource_id,
</span></span><span style="display:flex;"><span>    ScalableDimension<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;sagemaker:variant:DesiredInstanceCount&#34;</span>,
</span></span><span style="display:flex;"><span>    MinCapacity<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">0</span>,
</span></span><span style="display:flex;"><span>    MaxCapacity<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>response <span style="color:#ff7b72;font-weight:bold">=</span> client<span style="color:#ff7b72;font-weight:bold">.</span>put_scaling_policy(
</span></span><span style="display:flex;"><span>    PolicyName<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;Invocations-ScalingPolicy&#34;</span>,
</span></span><span style="display:flex;"><span>    ServiceNamespace<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;sagemaker&#34;</span>,  <span style="color:#8b949e;font-style:italic"># The namespace of the AWS service that provides the resource.</span>
</span></span><span style="display:flex;"><span>    ResourceId<span style="color:#ff7b72;font-weight:bold">=</span>resource_id,  <span style="color:#8b949e;font-style:italic"># Endpoint name</span>
</span></span><span style="display:flex;"><span>    ScalableDimension<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;sagemaker:variant:DesiredInstanceCount&#34;</span>,  <span style="color:#8b949e;font-style:italic"># SageMaker supports only Instance Count</span>
</span></span><span style="display:flex;"><span>    PolicyType<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;TargetTrackingScaling&#34;</span>,  <span style="color:#8b949e;font-style:italic"># &#39;StepScaling&#39;|&#39;TargetTrackingScaling&#39;</span>
</span></span><span style="display:flex;"><span>    TargetTrackingScalingPolicyConfiguration<span style="color:#ff7b72;font-weight:bold">=</span>{
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;TargetValue&#34;</span>: <span style="color:#a5d6ff">1.0</span>,  <span style="color:#8b949e;font-style:italic"># The target value for the metric. - here the metric is - SageMakerVariantInvocationsPerInstance</span>
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;PredefinedMetricSpecification&#34;</span>: {
</span></span><span style="display:flex;"><span>            <span style="color:#a5d6ff">&#34;PredefinedMetricType&#34;</span>: <span style="color:#a5d6ff">&#34;SageMakerVariantInvocationsPerInstance&#34;</span>
</span></span><span style="display:flex;"><span>        },
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;ScaleInCooldown&#34;</span>: <span style="color:#a5d6ff">2</span>,  <span style="color:#8b949e;font-style:italic"># The cooldown period helps you prevent your Auto Scaling group from launching or terminating</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b949e;font-style:italic"># additional instances before the effects of previous activities are visible.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b949e;font-style:italic"># You can configure the length of time based on your instance startup time or other application needs.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b949e;font-style:italic"># ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;ScaleOutCooldown&#34;</span>: <span style="color:#a5d6ff">2</span>,  <span style="color:#8b949e;font-style:italic"># ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b949e;font-style:italic"># &#39;DisableScaleIn&#39;: True|False - ndicates whether scale in by the target tracking policy is disabled.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#8b949e;font-style:italic"># If the value is true , scale in is disabled and the target tracking policy won&#39;t remove capacity from the scalable resource.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#a5d6ff">&#34;DisableScaleIn&#34;</span>: <span style="color:#79c0ff">False</span> 
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="aws-sagemaker-batch-transform">
  AWS Sagemaker Batch Transform
  <a class="heading-link" href="#aws-sagemaker-batch-transform">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>
<p>For SageMaker Batch Transform, the Dockerfile is slightly different; see details here: <a href="https://github.com/Asad-Ismail/ml-model-deployment/blob/main/sm_batch_transform/Dockerfile"  class="external-link" target="_blank" rel="noopener">Doclerfile_BT</a></p>
</li>
<li>
<p>Building and registering the model steps stay the same; also given here: <a href="https://github.com/Asad-Ismail/ml-model-deployment/blob/main/sm_batch_transform/Dockerfile"  class="external-link" target="_blank" rel="noopener">Build&amp;Register</a></p>
</li>
<li>
<p>Create SageMaker Batch Transform job:</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sm_cli <span style="color:#ff7b72;font-weight:bold">=</span> sagemaker_session<span style="color:#ff7b72;font-weight:bold">.</span>sagemaker_client
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>transformer<span style="color:#ff7b72;font-weight:bold">=</span>model<span style="color:#ff7b72;font-weight:bold">.</span>transformer(instance_count<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">1</span>, instance_type<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;ml.p3.2xlarge&#34;</span>, output_path<span style="color:#ff7b72;font-weight:bold">=</span>model_outputs_path,max_payload<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">100</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>transformer<span style="color:#ff7b72;font-weight:bold">.</span>transform(
</span></span><span style="display:flex;"><span>    data<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#39;s3://path_to_your_images/input/&#39;</span>, 
</span></span><span style="display:flex;"><span>    data_type<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;S3Prefix&#34;</span>,
</span></span><span style="display:flex;"><span>    content_type<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#a5d6ff">&#34;application/x-image&#34;</span>,
</span></span><span style="display:flex;"><span>    wait<span style="color:#ff7b72;font-weight:bold">=</span><span style="color:#79c0ff">True</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>job_info <span style="color:#ff7b72;font-weight:bold">=</span> sm_cli<span style="color:#ff7b72;font-weight:bold">.</span>describe_transform_job(TransformJobName<span style="color:#ff7b72;font-weight:bold">=</span>transformer<span style="color:#ff7b72;font-weight:bold">.</span>latest_transform_job<span style="color:#ff7b72;font-weight:bold">.</span>name)
</span></span></code></pre></div><h2 id="additional-comments-and-conclusion">
  Additional Comments and Conclusion
  <a class="heading-link" href="#additional-comments-and-conclusion">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>We have explored various strategies for deploying machine learning models, focusing on AWS services like SageMaker for real-time endpoints, asynchronous inference, and batch transform jobs. By using a practical example of instance segmentation with Detectron2&rsquo;s Mask R-CNN, We have demostrated following poiints:</p>
<ul>
<li>Flexibility: Custom containers allow you to deploy a wide range of models, not limited to specific frameworks.</li>
<li>Scalability: AWS services like SageMaker provide robust scaling options to handle varying workloads.</li>
<li>Performance: Options like real-time endpoints ensure low-latency responses for time-sensitive applications.
Limitations and Considerations:</li>
</ul>
<p>We did not consider and some topics for future are:</p>
<ul>
<li>
<p>Deployment Strategies: In real-world scenarios, you&rsquo;d often want to gradually shift traffic to new endpoints to minimize risks. Techniques like canary deployments and A/B testing allow you to test new models on a small percentage of traffic before full rollout.</p>
</li>
<li>
<p>Continuous Integration/Continuous Deployment (CI/CD): Automate your deployment pipeline for efficient updates and scaling.
Advanced Deployment Strategies: Explore canary deployments, blue/green deployments, and A/B testing to improve deployment safety and effectiveness.
By understanding these deployment strategies and their strengths and limitations, you can make informed decisions on how best to serve your machine learning models to meet your application&rsquo;s needs.</p>
</li>
</ul>
<p>Feel free to reach out or consult the <a href="https://github.com/Asad-Ismail/ml-model-deployment/blob/main/sm_batch_transform/batch_transform.ipynb"  class="external-link" target="_blank" rel="noopener">Repo</a> for the complete code.</p>
<h3 id="references">
  References
  <a class="heading-link" href="#references">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p><strong>AWS SageMaker Documentation:</strong></p>
<ul>
<li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html"  class="external-link" target="_blank" rel="noopener">Real-Time Inference</a></li>
<li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html"  class="external-link" target="_blank" rel="noopener">Asynchronous Inference</a></li>
<li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html"  class="external-link" target="_blank" rel="noopener">Batch Transform</a></li>
<li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html"  class="external-link" target="_blank" rel="noopener">Serverless Inference</a></li>
</ul>
<p><strong>Detectron2 Resources:</strong></p>
<ul>
<li><a href="https://github.com/facebookresearch/detectron2"  class="external-link" target="_blank" rel="noopener">Detectron2 GitHub Repository</a></li>
<li><a href="https://detectron2.readthedocs.io/"  class="external-link" target="_blank" rel="noopener">Detectron2 Documentation</a></li>
</ul>
<p><strong>AWS Autoscaling:</strong></p>
<ul>
<li><a href="https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html"  class="external-link" target="_blank" rel="noopener">Application Auto Scaling Documentation</a></li>
<li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-endpoints-auto-scaling.html"  class="external-link" target="_blank" rel="noopener">Scaling SageMaker Endpoints</a></li>
</ul>
<p><strong>Advanced Deployment Strategies:</strong></p>
<ul>
<li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green-canary.html"  class="external-link" target="_blank" rel="noopener">Canary Deployments with AWS SageMaker</a></li>
<li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html"  class="external-link" target="_blank" rel="noopener">Blue/Green Deployments</a></li>
</ul>

      </div>


      <footer>
        


        
        
        
        
        

        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.js"></script>
  

  

  


  
  



  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>

</html>
