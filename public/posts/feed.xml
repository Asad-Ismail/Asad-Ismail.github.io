<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Asad Ismail</title>
    <link>http://localhost:50071/posts/</link>
    <description>Recent content in Posts on Asad Ismail</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:50071/posts/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why Your Prompt Cache Keeps Missing</title>
      <link>http://localhost:50071/posts/prompt_caching/</link>
      <pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:50071/posts/prompt_caching/</guid>
      <description>&lt;h2 id=&#34;why-you-should-care&#34;&gt;&#xA;  Why you should care&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#why-you-should-care&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Agentic workflows resend a large, mostly static context on every call: system instructions, tool schemas, and growing conversation history. Prompt caching can make those repeated input tokens far cheaper and reduce latency. But the failure modes are subtle: small changes can shift where your prompt diverges in the cached prefix and wipe most (or all) cached reuse, raising both cost and latency.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ML Model Deployment on AWS: A Practical Guide</title>
      <link>http://localhost:50071/posts/mlmodeldeployment/</link>
      <pubDate>Sat, 11 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:50071/posts/mlmodeldeployment/</guid>
      <description>&lt;h1 id=&#34;ml-model-deployment-on-aws&#34;&gt;&#xA;  ML Model Deployment on AWS&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#ml-model-deployment-on-aws&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;Most ML projects should start with batch processing. It is cheaper, easier to debug, and forces you to think about whether you actually need real-time predictions. Only move to real-time endpoints when batch genuinely does not work for your use case.&lt;/p&gt;&#xA;&lt;p&gt;This guide covers four deployment patterns with working AWS code.&lt;/p&gt;&#xA;&lt;h2 id=&#34;four-deployment-patterns&#34;&gt;&#xA;  Four Deployment Patterns&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#four-deployment-patterns&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Pick based on your latency and cost requirements:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
